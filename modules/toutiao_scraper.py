import os
import asyncio
import json
import logging
from typing import List, Dict, Any, Optional
from playwright.async_api import Page
from .browser_manager import BrowserManager
from .image_handler import ImageHandler
from .qiniu_config import QiniuConfig
import traceback


class ToutiaoScraper:
    """åŸºäºPlaywrightçš„ä»Šæ—¥å¤´æ¡æŠ“å–å™¨"""
    
    def __init__(self, gui_config: Dict[str, Any], browser_manager: BrowserManager):
        self.browser_manager = browser_manager
        self.setup_logging()
        
        # åŠ è½½åŸºç¡€é…ç½®æ–‡ä»¶
        local_config = self.load_config('toutiao_config.json')
        
        # æ˜¾å¼åœ°ã€å®‰å…¨åœ°åˆå¹¶GUIé…ç½®ï¼Œé¿å…è¦†ç›–å¤æ‚å­—å…¸
        simple_keys_to_update = ['article_count', 'image_count']
        for key in simple_keys_to_update:
            if key in gui_config:
                local_config[key] = gui_config[key]
        
        self.config = local_config
        self.logger.info("ä»Šæ—¥å¤´æ¡æŠ“å–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        self.logger = logging.getLogger('modules.toutiao_scraper')
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
            self.logger.propagate = False
    
    async def scrape_articles_and_images(self, keyword: str, scrape_articles: bool = True, scrape_images: bool = True) -> bool:
        """
        å…¬å¼€çš„ä¸»æ–¹æ³•ï¼Œç”¨äºæ ¹æ®éœ€è¦æŠ“å–æ–‡ç« å’Œ/æˆ–å›¾ç‰‡é“¾æ¥ã€‚
        """
        try:
            if not await self.navigate_to_toutiao():
                self.logger.error("æ— æ³•æ‰“å¼€ä»Šæ—¥å¤´æ¡é¦–é¡µï¼ŒæŠ“å–ä»»åŠ¡ä¸­æ­¢ã€‚")
                return False

            if scrape_articles:
                self._clear_file("article.txt")
            if scrape_images:
                self._clear_file("picture.txt")

            articles_data = await self.search_articles(keyword, self.config.get('article_count', 5))
            
            if not articles_data:
                self.logger.warning(f"æœªèƒ½æ ¹æ®å…³é”®è¯ '{keyword}' æŠ“å–åˆ°ä»»ä½•æ–‡ç« æ•°æ®ã€‚")
                return False

            if scrape_articles:
                self._save_articles_content(articles_data)

            if scrape_images:
                await self._save_images_links(articles_data)
            
            self.logger.info("å¤´æ¡æŠ“å–å·¥ä½œæµç¨‹æˆåŠŸå®Œæˆã€‚")
            return True

        except Exception as e:
            self.logger.error(f"å¤´æ¡æŠ“å–å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
            return False

    def _save_articles_content(self, articles_data: List[Dict[str, Any]]):
        """ä»æŠ“å–çš„æ•°æ®ä¸­æå–å¹¶ä¿å­˜æ–‡ç« å†…å®¹ã€‚"""
        article_texts = [article['content'] for article in articles_data if article.get('content')]
        if article_texts:
            with open("article.txt", "w", encoding='utf-8') as f:
                f.write("\n\n---\n\n".join(article_texts))
            self.logger.info(f"å·²å°† {len(article_texts)} ç¯‡æ–‡ç« å†…å®¹ä¿å­˜åˆ° article.txt")
        else:
            self.logger.info("æŠ“å–åˆ°çš„æ–‡ç« å†…å®¹ä¸ºç©ºã€‚")

    async def _save_images_links(self, articles_data: List[Dict[str, Any]]):
        """
        ä»æŠ“å–çš„æ•°æ®ä¸­æå–ã€å¤„ç†å›¾ç‰‡å¹¶ä¿å­˜ä¸ƒç‰›äº‘é“¾æ¥ã€‚
        """
        all_images_with_referer = [img for article in articles_data for img in article.get('images_with_referer', [])]
        max_images = self.config.get('image_count', 3)
        images_to_process = all_images_with_referer[:max_images]

        if not images_to_process:
            self.logger.info("æœªæŠ“å–åˆ°ä»»ä½•å›¾ç‰‡é“¾æ¥ã€‚")
            return

        qiniu_loader = QiniuConfig()
        is_valid, message = qiniu_loader.validate()
        if not is_valid:
            self.logger.warning(message)
            return

        qiniu_config = qiniu_loader.get_config()
        # åªä¼ é€’ImageHandleréœ€è¦çš„å‚æ•°
        image_handler_config = {
            'access_key': qiniu_config.get('access_key'),
            'secret_key': qiniu_config.get('secret_key'),
            'bucket_name': qiniu_config.get('bucket_name'),
            'domain': qiniu_config.get('domain')
        }
        image_handler = ImageHandler(**image_handler_config)
        
        qiniu_links = []
        crop_pixels = self.config.get('scraping', {}).get('crop_bottom_pixels', 80)
        self.logger.info(f"å›¾ç‰‡å¤„ç†ï¼šå°†ä»æ¯å¼ å›¾ç‰‡åº•éƒ¨è£å‰ª {crop_pixels} åƒç´ ã€‚")
        
        for i, image_data in enumerate(images_to_process):
            try:
                url, referer = image_data['url'], image_data['referer']
                self.logger.info(f"--- [å›¾ç‰‡ {i+1}/{len(images_to_process)}] å¼€å§‹å¤„ç†: {url} ---")
                qiniu_link = image_handler.process_and_upload_image(url, crop_bottom_pixels=crop_pixels, referer=referer)
                if qiniu_link:
                    qiniu_links.append(qiniu_link)
                    self.logger.info(f"å›¾ç‰‡æˆåŠŸä¸Šä¼ åˆ°ä¸ƒç‰›äº‘: {qiniu_link}")
                else:
                    self.logger.warning(f"å›¾ç‰‡å¤„ç†æˆ–ä¸Šä¼ å¤±è´¥ï¼Œè·³è¿‡: {url}")
            except Exception as e:
                self.logger.error(f"å¤„ç†å•å¼ å›¾ç‰‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {url}, é”™è¯¯: {e}", exc_info=True)

        if qiniu_links:
            markdown_links = [f"![Image]({link})" for link in qiniu_links]
            with open("picture.txt", "w", encoding='utf-8') as f:
                f.write("\n".join(markdown_links))
            self.logger.info(f"å·²å°† {len(qiniu_links)} ä¸ªä¸ƒç‰›äº‘å›¾ç‰‡é“¾æ¥ï¼ˆMarkdownæ ¼å¼ï¼‰ä¿å­˜åˆ° picture.txt")
        else:
            self.logger.warning("æ‰€æœ‰å›¾ç‰‡å¤„ç†/ä¸Šä¼ å‡å¤±è´¥ï¼Œpicture.txt ä¸ºç©ºã€‚")

    def load_config(self, config_file: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            
            self.logger.info(f"é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºå¹¶ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
            default_config = {
                "selectors": {
                    "homepage": {"search_input": "input[type='search']", "search_button": "button[type='submit']"},
                    "search_results": {
                        "news_tab": "div[role='tablist'] a:has-text('èµ„è®¯')", 
                        "article_links": ["div.cs-view a[href*='/article/']", "//div[contains(@class, 'result')]//a[contains(@href, 'toutiao.com')]"],
                        "next_page_button": "button:has-text('ä¸‹ä¸€é¡µ')"
                    },
                    "article_page": {"content_containers": ["article", "div.article-content"], "title_selectors": ["h1.article-title", "h1"], "image_selectors": ["article img", "div.pgc-img img"]}
                },
                "timeouts": {"page_load": 30, "element_wait": 15, "search_delay": 5, "article_delay": 2},
                "scraping": {"max_articles": 10, "max_pages": 5, "max_images": 5, "delay_between_requests": 2, "content_min_length": 100, "crop_bottom_pixels": 80}
            }
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
            return default_config
                
        except Exception as e:
            self.logger.error(f"åŠ è½½æˆ–åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            return {}
    
    async def navigate_to_toutiao(self) -> bool:
        """å¯¼èˆªåˆ°ä»Šæ—¥å¤´æ¡"""
        try:
            await self.browser_manager.navigate("https://www.toutiao.com/")
            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç 
            has_verification = await self._check_for_captcha()
            if has_verification:
                self.logger.info("å·²å¤„ç†äººå·¥éªŒè¯ï¼Œç»§ç»­æ‰§è¡Œ...")
            self.logger.info("æˆåŠŸå¯¼èˆªåˆ°ä»Šæ—¥å¤´æ¡")
            return True
        except Exception as e:
            self.logger.error(f"å¯¼èˆªåˆ°ä»Šæ—¥å¤´æ¡å¤±è´¥: {e}", exc_info=True)
            return False
    
    async def search_articles(self, keyword: str, max_articles: int = 5) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨Playwrightè¿›è¡Œæœç´¢å’ŒæŠ“å–ï¼Œå¹¶å¤„ç†ç¿»é¡µï¼Œç›´åˆ°æ»¡è¶³æ•°é‡è¦æ±‚ã€‚
        """
        self.logger.info(f"å¼€å§‹æœç´¢å…³é”®è¯: '{keyword}', ç›®æ ‡æ–‡ç« æ•°: {max_articles}")
        page = self.browser_manager.page
        if not page:
            self.logger.error("ä¸»é¡µé¢æœªåˆå§‹åŒ–ã€‚")
            return []

        # 1. æ‰§è¡Œåˆå§‹æœç´¢ï¼Œè¿›å…¥æœç´¢ç»“æœé¡µ
        selectors = self.config.get('selectors', {})
        search_results_page = None
        try:
            self.logger.info("å‡†å¤‡åœ¨åŸå§‹é¡µé¢æ‰§è¡Œæœç´¢...")
            async with page.context.expect_page() as new_page_info:
                # ä½¿ç”¨ä¿®å¤åçš„å…ƒç´ å®šä½æ–¹æ³•
                search_input_selector = selectors['homepage']['search_input']
                search_button_selector = selectors['homepage']['search_button']
                
                # æ”¯æŒXPathé€‰æ‹©å™¨
                if search_input_selector.startswith('//') or search_input_selector.startswith('/'):
                    search_input = page.locator(f"xpath={search_input_selector}")
                else:
                    search_input = page.locator(search_input_selector)
                
                if search_button_selector.startswith('//') or search_button_selector.startswith('/'):
                    search_button = page.locator(f"xpath={search_button_selector}")
                else:
                    search_button = page.locator(search_button_selector)
                
                await search_input.fill(keyword)
                await search_button.click()
            
            search_results_page = await new_page_info.value
            await search_results_page.wait_for_load_state()
            self.logger.info("å·²æ•è·æœç´¢ç»“æœæ–°æ ‡ç­¾é¡µã€‚")
            
            # åœ¨æœç´¢ç»“æœé¡µé¢ä¹Ÿæ£€æŸ¥éªŒè¯ç 
            current_page = self.browser_manager.page
            self.browser_manager.page = search_results_page  # ä¸´æ—¶åˆ‡æ¢é¡µé¢è¿›è¡Œæ£€æŸ¥
            has_verification = await self._check_for_captcha()
            self.browser_manager.page = current_page  # æ¢å¤åŸé¡µé¢
            
            if has_verification:
                self.logger.info("æœç´¢ç»“æœé¡µé¢éªŒè¯å·²å¤„ç†ï¼Œç»§ç»­æ‰§è¡Œ...")

            # ç‚¹å‡»èµ„è®¯æ ‡ç­¾
            news_tab_selector = selectors['search_results']['news_tab']
            if news_tab_selector.startswith('//') or news_tab_selector.startswith('/'):
                news_tab = search_results_page.locator(f"xpath={news_tab_selector}")
            else:
                news_tab = search_results_page.locator(news_tab_selector)
            
            await news_tab.click()
            self.logger.info("å·²ç‚¹å‡»'èµ„è®¯'æ ‡ç­¾ï¼Œç­‰å¾…æ–‡ç« åˆ—è¡¨åŠ è½½...")
            await asyncio.sleep(2)

            # 2. å¾ªç¯æŠ“å–å’Œç¿»é¡µ
            all_articles_data = []
            page_count = 0
            max_pages_to_scrape = self.config.get('scraping', {}).get('max_pages', 5)

            while len(all_articles_data) < max_articles and page_count < max_pages_to_scrape:
                page_count += 1
                self.logger.info(f"--- å¼€å§‹æŠ“å–ç¬¬ {page_count} é¡µ ---")
                
                # ä¼ é€’å‰©ä½™éœ€è¦æŠ“å–çš„æ•°é‡ï¼Œä½†_scrape_current_pageä¼šå°è¯•æŠ“å–å½“å‰é¡µé¢æ‰€æœ‰é“¾æ¥
                remaining_needed = max_articles - len(all_articles_data)
                new_data = await self._scrape_current_page(search_results_page, remaining_needed)
                if new_data:
                    all_articles_data.extend(new_data)
                    self.logger.info(f"ç¬¬ {page_count} é¡µæˆåŠŸæŠ“å– {len(new_data)} ç¯‡æ–‡ç« ï¼Œæ€»è®¡: {len(all_articles_data)}/{max_articles}")
                else:
                    self.logger.warning(f"ç¬¬ {page_count} é¡µæ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡ç« ")
                
                if len(all_articles_data) >= max_articles:
                    self.logger.info(f"å·²æˆåŠŸæŠ“å– {len(all_articles_data)} ç¯‡æ–‡ç« ï¼Œè¾¾åˆ°ç›®æ ‡æ•°é‡ã€‚")
                    break
                
                # å°è¯•ç¿»é¡µ
                next_button_selectors = selectors.get('search_results', {}).get('next_page_buttons')
                if not next_button_selectors:
                    self.logger.warning("é…ç½®ä¸­æœªæ‰¾åˆ°'ä¸‹ä¸€é¡µ'æŒ‰é’®é€‰æ‹©å™¨ï¼Œåœæ­¢ç¿»é¡µã€‚")
                    break

                # å°è¯•æ¯ä¸ªå¯èƒ½çš„ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
                next_button_found = False
                for next_button_selector in next_button_selectors:
                    try:
                        if next_button_selector.startswith('//') or next_button_selector.startswith('/'):
                            next_button = search_results_page.locator(f"xpath={next_button_selector}")
                        else:
                            next_button = search_results_page.locator(next_button_selector)
                        
                        if await next_button.is_visible():
                            self.logger.info(f"ç‚¹å‡»'ä¸‹ä¸€é¡µ'æŒ‰é’®... (ä½¿ç”¨é€‰æ‹©å™¨: {next_button_selector})")
                            await next_button.click()
                            await search_results_page.wait_for_load_state('domcontentloaded')
                            await asyncio.sleep(3) # ç­‰å¾…é¡µé¢å†…å®¹åˆ·æ–°
                            next_button_found = True
                            break
                    except Exception as e:
                        self.logger.debug(f"å°è¯•ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨å¤±è´¥: {next_button_selector}, é”™è¯¯: {e}")
                        continue
                
                if not next_button_found:
                    self.logger.info("æœªæ‰¾åˆ°å¯ç”¨çš„'ä¸‹ä¸€é¡µ'æŒ‰é’®ï¼ŒæŠ“å–ç»“æŸã€‚")
                    break
            
            return all_articles_data

        except Exception as e:
            self.logger.error(f"æœç´¢æ–‡ç« æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return []
        finally:
            if search_results_page and not search_results_page.is_closed():
                await search_results_page.close()
                self.logger.info("æœç´¢ç»“æœæ ‡ç­¾é¡µå·²å…³é—­ã€‚")

    async def _scrape_current_page(self, page: Page, max_count: int) -> List[Dict[str, Any]]:
        """ä»å½“å‰é¡µé¢æå–æ–‡ç« æ•°æ®ï¼Œä¼šä¾æ¬¡å°è¯•é…ç½®æ–‡ä»¶ä¸­æä¾›çš„å¤šä¸ªé€‰æ‹©å™¨ã€‚"""
        possible_selectors = self.config.get('selectors', {}).get('search_results', {}).get('article_links')
        
        if not isinstance(possible_selectors, list):
            self.logger.error("é…ç½®é”™è¯¯ï¼š'article_links' åº”è¯¥æ˜¯ä¸€ä¸ªé€‰æ‹©å™¨åˆ—è¡¨ï¼ˆlistï¼‰ã€‚")
            return []

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„é€‰æ‹©å™¨
        valid_selector = None
        count = 0
        for selector in possible_selectors:
            self.logger.info(f"æ­£åœ¨å°è¯•ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
            try:
                # æ”¯æŒXPathé€‰æ‹©å™¨
                if selector.startswith('//') or selector.startswith('/'):
                    await page.wait_for_selector(f"xpath={selector}", state='attached', timeout=5000)
                    count = await page.locator(f"xpath={selector}").count()
                else:
                    await page.wait_for_selector(selector, state='attached', timeout=5000)
                    count = await page.locator(selector).count()
                
                if count > 0:
                    self.logger.info(f"é€‰æ‹©å™¨ '{selector}' æˆåŠŸæ‰¾åˆ° {count} ä¸ªé“¾æ¥ã€‚")
                    valid_selector = selector
                    break
            except Exception:
                self.logger.warning(f"é€‰æ‹©å™¨ '{selector}' å¤±è´¥æˆ–è¶…æ—¶ï¼Œå°è¯•ä¸‹ä¸€ä¸ªã€‚")
        
        if not valid_selector:
            self.logger.error("æ‰€æœ‰å¤‡é€‰é€‰æ‹©å™¨éƒ½æœªèƒ½æ‰¾åˆ°æ–‡ç« é“¾æ¥ã€‚")
            return []

        try:
            # ä¿®å¤é€»è¾‘ï¼šå…ˆå°è¯•æŠ“å–å½“å‰é¡µé¢çš„æ‰€æœ‰é“¾æ¥ï¼Œè€Œä¸æ˜¯åªæŠ“å–max_countä¸ª
            all_articles_data = []
            self.logger.info(f"å½“å‰é¡µé¢å…±æ‰¾åˆ° {count} ä¸ªé“¾æ¥ï¼Œå°†é€ä¸ªå°è¯•æŠ“å–ï¼ˆè·³è¿‡å†…å®¹å¤ªçŸ­çš„æ–‡ç« ï¼‰ã€‚")

            for i in range(count):  # éå†æ‰€æœ‰é“¾æ¥ï¼Œè€Œä¸æ˜¯é™åˆ¶æ•°é‡
                # å¦‚æœå·²ç»æŠ“å–åˆ°è¶³å¤Ÿçš„æ–‡ç« ï¼Œåœæ­¢æŠ“å–
                if len(all_articles_data) >= max_count:
                    self.logger.info(f"å·²æŠ“å–åˆ° {len(all_articles_data)} ç¯‡æœ‰æ•ˆæ–‡ç« ï¼Œè¾¾åˆ°å½“å‰é¡µé¢ç›®æ ‡æ•°é‡ã€‚")
                    break
                    
                self.logger.info(f"--- å‡†å¤‡å¤„ç†ç¬¬ {i + 1}/{count} ä¸ªé“¾æ¥ ---")
                
                # åœ¨æ¯æ¬¡äº¤äº’å‰é‡æ–°å®šä½å…ƒç´ ï¼Œç¡®ä¿è·å–åˆ°çš„æ˜¯æœ€æ–°çš„çŠ¶æ€
                if valid_selector.startswith('//') or valid_selector.startswith('/'):
                    current_link_locator = page.locator(f"xpath={valid_selector}").nth(i)
                else:
                    current_link_locator = page.locator(valid_selector).nth(i)
                
                # ç­‰å¾…å…ƒç´ å¯è§
                try:
                    await current_link_locator.wait_for(state='visible', timeout=10000)
                except Exception as e:
                    self.logger.warning(f"ç¬¬ {i+1} ä¸ªé“¾æ¥å…ƒç´ ä¸å¯è§ï¼Œè·³è¿‡: {e}")
                    continue
                
                href = await current_link_locator.get_attribute('href')
                if not href:
                    self.logger.warning(f"ç¬¬ {i+1} ä¸ªé“¾æ¥æ²¡æœ‰hrefå±æ€§ï¼Œè·³è¿‡ã€‚")
                    continue
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if href.startswith('/'):
                    href = f"https://www.toutiao.com{href}"
                elif not href.startswith('http'):
                    href = f"https://www.toutiao.com/{href}"
                
                self.logger.info(f"ç¬¬ {i+1} ä¸ªé“¾æ¥åœ°å€: {href}")

                # ç‚¹å‡»é“¾æ¥ï¼Œå¹¶ç­‰å¾…æ–°é¡µé¢
                try:
                    async with page.context.expect_page(timeout=30000) as article_page_info:
                        # æ»šåŠ¨åˆ°å…ƒç´ å¹¶ç‚¹å‡»
                        await current_link_locator.scroll_into_view_if_needed()
                        await asyncio.sleep(0.5)
                        await current_link_locator.click()
                    
                    article_page = await article_page_info.value
                    await article_page.wait_for_load_state()
                    self.logger.info("å·²æ•è·æ–‡ç« è¯¦æƒ…æ–°æ ‡ç­¾é¡µã€‚")
                except Exception as e:
                    self.logger.error(f"ç‚¹å‡»ç¬¬ {i+1} ä¸ªé“¾æ¥å¤±è´¥: {e}")
                    continue

                try:
                    # åœ¨æ–°æ ‡ç­¾é¡µä¸­æå–å†…å®¹
                    article_data = await self.extract_article_content(article_page, article_page.url)
                    if article_data:
                        all_articles_data.append(article_data)
                finally:
                    # ç¡®ä¿æ–‡ç« é¡µè¢«å…³é—­
                    if not article_page.is_closed():
                        await article_page.close()
                        self.logger.info("æ–‡ç« è¯¦æƒ…æ ‡ç­¾é¡µå·²å…³é—­ï¼Œè¿”å›æœç´¢ç»“æœé¡µã€‚")
                
                await asyncio.sleep(self.config.get('scraping', {}).get('delay_between_requests', 2))

            return all_articles_data
        
        except Exception as e:
            self.logger.error(f"ä»ç»“æœé¡µé¢æå–æ–‡ç« é“¾æ¥æ—¶å‡ºé”™: {e}", exc_info=True)
            return []
    
    async def extract_article_content(self, page: Page, url: str) -> Optional[Dict[str, Any]]:
        """åœ¨æ–°æ ‡ç­¾é¡µä¸­æ‰“å¼€æ–‡ç« å¹¶æå–å†…å®¹"""
        try:
            # ç›´æ¥ä½¿ç”¨å·²ç»æ‰“å¼€çš„é¡µé¢ï¼Œä¸éœ€è¦å†æ¬¡å¯¼èˆª
            await page.wait_for_load_state('domcontentloaded', timeout=20000)
            await asyncio.sleep(self.config.get('timeouts', {}).get('article_delay', 2))

            title = await self._extract_text_by_selectors(page, self.config['selectors']['article_page']['title_selectors'])
            content = await self._extract_text_by_selectors(page, self.config['selectors']['article_page']['content_containers'])
            images = await self._extract_article_images_with_referer(page, self.config['selectors']['article_page']['image_selectors'])
            
            self.logger.info(f"åœ¨é¡µé¢ {url} æå–åˆ° - æ ‡é¢˜: '{title[:20] if title else 'N/A'}...', å†…å®¹é•¿åº¦: {len(content)}, å›¾ç‰‡æ•°é‡: {len(images)}")

            if content and len(content) > self.config.get('scraping', {}).get('content_min_length', 100):
                return {
                    'url': url,
                    'title': title or 'æ— æ ‡é¢˜',
                    'content': content,
                    'images_with_referer': images
                }
            else:
                self.logger.warning(f"æ–‡ç« å†…å®¹å¤ªçŸ­æˆ–ä¸ºç©ºï¼Œè·³è¿‡: {url}")
                return None
        except Exception as e:
            self.logger.error(f"æå–æ–‡ç« å†…å®¹å¤±è´¥: {url}, é”™è¯¯: {e}", exc_info=True)
            return None

    async def _extract_text_by_selectors(self, page: Page, selectors: List[str]) -> str:
        """æ ¹æ®é€‰æ‹©å™¨åˆ—è¡¨æå–ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡æœ¬å†…å®¹ã€‚"""
        for selector in selectors:
            try:
                # æ”¯æŒXPathé€‰æ‹©å™¨  
                if selector.startswith('//') or selector.startswith('/'):
                    locator = page.locator(f"xpath={selector}").first
                else:
                    locator = page.locator(selector).first
                    
                if await locator.is_visible(timeout=2000):
                    text = await locator.inner_text()
                    if text and text.strip():
                        # æ¸…ç†æ–‡æœ¬å†…å®¹ï¼Œç§»é™¤å›¾ç‰‡ç›¸å…³çš„å¤‡æ³¨
                        cleaned_text = self._clean_article_text(text.strip())
                        return cleaned_text
            except Exception as e:
                self.logger.debug(f"é€‰æ‹©å™¨ {selector} æå–æ–‡æœ¬å¤±è´¥: {e}")
                continue
        return ""
    
    def _clean_article_text(self, text: str) -> str:
        """æ¸…ç†æ–‡ç« æ–‡æœ¬ï¼Œç§»é™¤å›¾ç‰‡å¤‡æ³¨ç­‰æ— å…³å†…å®¹"""
        import re
        
        # ç§»é™¤å¸¸è§çš„å›¾ç‰‡å¤‡æ³¨æ–‡å­— - ä½¿ç”¨å¥å­è¾¹ç•Œæ¥ç²¾ç¡®åŒ¹é…
        patterns_to_remove = [
            r'å›¾ç‰‡æ¥æºäºç½‘ç»œ[ï¼Œã€‚ã€]*[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡æ¥æºï¼š[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾æºï¼š[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'é…å›¾æ¥æº[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'ï¼ˆå›¾ç‰‡æ¥æº[^ï¼‰]*ï¼‰',
            r'\(å›¾ç‰‡æ¥æº[^)]*\)',
            r'ã€å›¾ç‰‡æ¥æº[^ã€‘]*ã€‘',
            r'å›¾ç‰‡ç‰ˆæƒå½’[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡ä»…ä¾›å‚è€ƒ[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡ä¸å†…å®¹æ— å…³[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡ä¸ºé…å›¾[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'ç½‘ç»œé…å›¾[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡æ¥è‡ªç½‘ç»œ[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡ç´ ææ¥æº[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡æ¥æºç½‘ç»œ[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡æ¥æºï¼šç½‘ç»œ[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡æ¥æºäºç½‘ç»œï¼Œå¦‚æœ‰ä¾µæƒè¯·è”ç³»åˆ é™¤[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡æ¥æºç½‘ç»œï¼Œå¦‚æœ‰ä¾µæƒè¯·è”ç³»åˆ é™¤[^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡æ¥æºäºç½‘ç»œï¼Œä¾µåˆ [^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?',
            r'å›¾ç‰‡æ¥æºç½‘ç»œï¼Œä¾µåˆ [^ã€‚ï¼ï¼Ÿ\n]*[ã€‚ï¼ï¼Ÿ]?'
        ]
        
        cleaned_text = text
        for pattern in patterns_to_remove:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)
        
        # æ¸…ç†ç©ºçš„æ‹¬å·å’Œæ–¹æ‹¬å·
        cleaned_text = re.sub(r'ï¼ˆ\s*ï¼‰', '', cleaned_text)
        cleaned_text = re.sub(r'ã€\s*ã€‘', '', cleaned_text)
        cleaned_text = re.sub(r'\(\s*\)', '', cleaned_text)
        cleaned_text = re.sub(r'\[\s*\]', '', cleaned_text)
        
        # ä¿®å¤å› åˆ é™¤å†…å®¹å¯¼è‡´çš„è¯­æ³•é—®é¢˜
        cleaned_text = re.sub(r'(\w+)ï¼ˆ\s*(\w+)', r'\1\2', cleaned_text)
        
        # æ¸…ç†æ ‡ç‚¹ç¬¦å·
        cleaned_text = re.sub(r'[ï¼Œã€‚ã€]*\s*[ï¼Œã€‚ã€]+', 'ï¼Œ', cleaned_text)
        cleaned_text = re.sub(r'ï¼Œ\s*ã€‚', 'ã€‚', cleaned_text)
        cleaned_text = re.sub(r'ï¼Œ\s*ï¼Œ', 'ï¼Œ', cleaned_text)
        cleaned_text = re.sub(r'ã€‚\s*ã€‚', 'ã€‚', cleaned_text)
        
        # æ¸…ç†ç©ºæ ¼å’Œæ¢è¡Œ
        cleaned_text = re.sub(r'\n\s*\n', '\n\n', cleaned_text)
        cleaned_text = re.sub(r'[ \t]+', ' ', cleaned_text)
        
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„å¤šä½™æ ‡ç‚¹
        cleaned_text = re.sub(r'^[ï¼Œã€‚ã€\s]+', '', cleaned_text)
        cleaned_text = re.sub(r'[ï¼Œã€‚ã€\s]+$', '', cleaned_text)
        
        # ç¡®ä¿å¥å­ä»¥æ­£ç¡®çš„æ ‡ç‚¹ç»“å°¾
        if cleaned_text and not cleaned_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
            cleaned_text += 'ã€‚'
        
        return cleaned_text.strip()
    
    async def _extract_article_images_with_referer(self, page: Page, selectors: List[str]) -> List[Dict[str, str]]:
        """æ ¹æ®é€‰æ‹©å™¨åˆ—è¡¨æå–æ‰€æœ‰åŒ¹é…çš„å›¾ç‰‡é“¾æ¥ã€‚"""
        images = []
        referer_url = page.url
        for selector in selectors:
            try:
                # æ”¯æŒXPathé€‰æ‹©å™¨
                if selector.startswith('//') or selector.startswith('/'):
                    locator = page.locator(f"xpath={selector}")
                else:
                    locator = page.locator(selector)
                
                # æ£€æŸ¥é€‰æ‹©å™¨æœ¬èº«æ˜¯å¦å­˜åœ¨äºé¡µé¢ä¸Š
                if await locator.count() > 0:
                    for img_locator in await locator.all():
                        try:
                            if src := await img_locator.get_attribute('src'):
                                if src.startswith('http'):
                                    images.append({'url': src, 'referer': referer_url})
                                elif src.startswith('//'):
                                    images.append({'url': f'https:{src}', 'referer': referer_url})
                        except Exception as e:
                            self.logger.debug(f"æå–å›¾ç‰‡é“¾æ¥å¤±è´¥: {e}")
                            continue
                    # å¦‚æœè¿™ä¸ªé€‰æ‹©å™¨æ‰¾åˆ°äº†å›¾ç‰‡ï¼Œå°±æ²¡å¿…è¦å†è¯•å…¶ä»–çš„äº†
                    if images:
                        return images
            except Exception as e:
                self.logger.debug(f"é€‰æ‹©å™¨ {selector} æŸ¥æ‰¾å›¾ç‰‡å¤±è´¥: {e}")
                continue
        return images
    
    async def cleanup(self):
        """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
        self.logger.info("ToutiaoScraperæ­£åœ¨æ‰§è¡Œæ¸…ç†æ“ä½œ...")
        if self.browser_manager:
            await self.browser_manager.cleanup()

    async def _check_for_captcha(self):
        """æ£€æŸ¥é¡µé¢æ˜¯å¦å‡ºç°éªŒè¯ç æˆ–äººå·¥éªŒè¯"""
        try:
            page = self.browser_manager.page
            if not page:
                return
            
            # ä»é…ç½®æ–‡ä»¶è·å–éªŒè¯é€‰æ‹©å™¨
            verification_selectors = self.config.get('verification', {}).get('selectors', [])
            
            self.logger.info("æ­£åœ¨æ£€æŸ¥æ˜¯å¦å‡ºç°äººå·¥éªŒè¯...")
            
            for selector in verification_selectors:
                try:
                    if selector.startswith('//') or selector.startswith('/'):
                        locator = page.locator(f"xpath={selector}")
                    else:
                        locator = page.locator(selector)
                    
                    if await locator.is_visible(timeout=2000):
                        self.logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°äººå·¥éªŒè¯å…ƒç´ : {selector}")
                        await self._handle_manual_verification(selector)
                        return True
                        
                except Exception as e:
                    self.logger.debug(f"æ£€æŸ¥éªŒè¯å…ƒç´  {selector} æ—¶å‡ºé”™: {e}")
                    continue
            
            self.logger.info("âœ… æœªæ£€æµ‹åˆ°äººå·¥éªŒè¯ï¼Œç»§ç»­æ‰§è¡Œ...")
            return False
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥éªŒè¯ç æ—¶å‡ºç°å¼‚å¸¸: {e}")
            return False

    async def _handle_manual_verification(self, detected_selector: str):
        """å¤„ç†äººå·¥éªŒè¯"""
        self.logger.warning("=" * 60)
        self.logger.warning("ğŸš¨ æ£€æµ‹åˆ°ä»Šæ—¥å¤´æ¡äººå·¥éªŒè¯ï¼")
        self.logger.warning(f"æ£€æµ‹åˆ°çš„éªŒè¯å…ƒç´ : {detected_selector}")
        self.logger.warning("=" * 60)
        self.logger.warning("ğŸ“‹ è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
        self.logger.warning("1. åœ¨æµè§ˆå™¨ä¸­å®Œæˆäººå·¥éªŒè¯ï¼ˆæ»‘å—ã€ç‚¹å‡»å›¾ç‰‡ç­‰ï¼‰")
        self.logger.warning("2. ç­‰å¾…éªŒè¯é€šè¿‡ï¼Œé¡µé¢æ­£å¸¸æ˜¾ç¤º")
        self.logger.warning("3. å®Œæˆååœ¨æ§åˆ¶å°æŒ‰å›è½¦é”®ç»§ç»­...")
        self.logger.warning("=" * 60)
        
        # æš‚åœæ‰§è¡Œï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å¤„ç†
        try:
            input("âŒ¨ï¸  è¯·å®ŒæˆéªŒè¯åæŒ‰å›è½¦é”®ç»§ç»­...")
            self.logger.info("âœ… ç”¨æˆ·ç¡®è®¤å·²å®ŒæˆéªŒè¯ï¼Œç»§ç»­æ‰§è¡Œ...")
            
            # ç­‰å¾…éªŒè¯å®Œæˆåé¡µé¢åŠ è½½
            await asyncio.sleep(3)
            
            # å†æ¬¡æ£€æŸ¥éªŒè¯æ˜¯å¦çœŸçš„å®Œæˆäº†
            page = self.browser_manager.page
            if detected_selector.startswith('//') or detected_selector.startswith('/'):
                locator = page.locator(f"xpath={detected_selector}")
            else:
                locator = page.locator(detected_selector)
            
            if await locator.is_visible(timeout=5000):
                self.logger.warning("âš ï¸  éªŒè¯å…ƒç´ ä»ç„¶å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦é‡æ–°éªŒè¯")
                # é€’å½’è°ƒç”¨ï¼Œå†æ¬¡å¤„ç†
                await self._handle_manual_verification(detected_selector)
            else:
                self.logger.info("âœ… éªŒè¯å·²å®Œæˆï¼ŒéªŒè¯å…ƒç´ å·²æ¶ˆå¤±")
                
        except KeyboardInterrupt:
            self.logger.error("âŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
            raise
        except Exception as e:
            self.logger.error(f"å¤„ç†äººå·¥éªŒè¯æ—¶å‡ºé”™: {e}")
            raise
    
    def _clear_file(self, filename: str):
        """å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œåˆ™æ¸…ç©ºå®ƒ"""
        if os.path.exists(filename):
            try:
                os.remove(filename)
                self.logger.info(f"å·²æ¸…ç†æ—§æ–‡ä»¶: {filename}")
            except OSError as e:
                self.logger.error(f"æ¸…ç†æ–‡ä»¶ {filename} å¤±è´¥: {e}")